apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Release.Name }}-configmap
data:
  kibana.yml: |-
    server:
      host: "0"
      port: 5601
      name: "kibana.default.svc"
      ssl:
        enabled: true
        certificate: "/usr/share/kibana/data/elasticsearch.cer"
        key: "/usr/share/kibana/data/elasticsearch.key"
    elasticsearch.hosts: ["https://elasticsearch-client.default.svc:443"]
    elasticsearch.username: "kibana"
    xpack.monitoring.elasticsearch.username: "elastic"
    xpack.monitoring.elasticsearch.password: "es_bootstrap_password"
    elasticsearch.ssl.verificationMode: "certificate"
    #elasticsearch.ssl.key: "/usr/share/kibana/data/certs/elasticsearch.key"
    #elasticsearch.ssl.certificate: "/usr/share/kibana/data/certs/elasticsearch.cer"
    elasticsearch.ssl.certificateAuthorities: ["/usr/share/kibana/data/elasticsearch-ca.pem"]
    elasticsearch.requestTimeout: 90000
    xpack:
      security:
        loginAssistanceMessage: "OptumHealth Technology Operations Center."

  apm-server.yml: |-
    apm-server:
      host: "0.0.0.0:8200"
      kibana:
        enabled: true
        host: "https://kibana.default.svc:443"
        username: "elastic"
        password: "${ES_APMSERVER_PASSWORD}"
    output.elasticsearch:
      hosts: ["https://elasticsearch-client.default.svc:443"]
      username: "elastic"
      #password: "es_bootstrap_password"
      password: "${ES_APMSERVER_PASSWORD}"
      ssl:
        enabled: "true"
        verification_mode: "full"
        certificate_authorities: ["/usr/share/apm-server/data/certs/elasticsearch-ca.pem"]
    monitoring.enabled: true
    monitoring.elasticsearch:
      hosts: ["https://elasticsearch-client.default.svc:443"]
    apm-server.rum.enabled: true


  fluent.conf: |-
    <system>
      log_level debug
    </system>

    <source>
      @type monitor_agent
      bind 0.0.0.0
      port 24220
    </source>

    #<source>
      #@type ping_message
      #@label @heartbeat_events
      #tag      ping
      #interval 10
      #data     "this is ping message"
      #<inject>
        #hostname_key  "host": "my.hostname.example.com"
        #time_key     time
        #time_type    string
        #time_format  "%Y-%m-%d %H:%M:%S" # {"time": "2017-02-01 14:50:38"}
        #timezone     "localtime yes" # or "localtime yes" / "localtime no" (UTC), ...
      #</inject>
    #</source>

    <source>
      @type http
      bind 0.0.0.0
      port 9880
      add_http_headers true
    </source>

    <source>
      @type forward
      bind 0.0.0.0
      port 24224
      tag forward
    </source>

    <match fluentbit>
      @type rewrite_tag_filter
      <rule>
        key HTTP_FLUENT_TAG
        pattern /^(.*)$/
        tag $1
      </rule>
    </match>

    #<filter logs>
      #@type record_transformer
      #enable_ruby
      #<record>
        #new_tag ${record["log"].gsub(/.*/,/trace.id=(.*\d)\,/)}
        #new_tag db2b9a1c9a72a7eef3fe451a64e72e3a
      #</record>
    #</filter>

    <match metrics>
      @type elasticsearch
      host elasticsearch-client.default.svc
      port 80
      user elastic
      password es_bootstrap_password
      index_name metrics
      type_name metrics
      emit_error_for_missing_id true
      logstash_format true
      logstash_prefix metrics
      <buffer>
        flush_thread_count 8
      </buffer>
    </match>

    <match logs>
      @type elasticsearch
      host elasticsearch-client.default.svc
      port 80
      user elastic
      password es_bootstrap_password
      index_name logs
      type_name logs
      emit_error_for_missing_id true
      logstash_format true
      logstash_prefix logs
      <buffer>
        flush_thread_count 8
      </buffer>
      #<parse>
        #@type regexp
        #expression (?<first>.{4}).*trace\.id\=(?<trace_id>.+),
        #time_key log_time
      #</parse>
    </match>

    #<match **>
      #@type splunk_hec
      #hec_host 12.34.56.78
      #hec_port 8088
      #hec_token 00000000-0000-0000-0000-000000000000
    #</match>

    #<label @heartbeat_events>
      ##=> tag: 'ping'
      ##   message: {'data' => 'your.hostname.local'}
      #<match ping>
        ## send hosts w/ ping_message_checker
        #@type stdout
      #</match>
    #</label>

    <match forward>
      @type stdout
    </match>

  metricbeat.yml: |-
    # ======================== Metricbeat Configuration =========================
    metricbeat.config.modules:

      # Glob pattern for configuration reloading
      path: ${path.config}/modules.d/*.yml

      # Period on which files under path should be checked for changes
      reload.period: 10s

      # Set to true to enable config reloading
      reload.enabled: false

    # Maximum amount of time to randomly delay the start of a metricset. Use 0 to
    # disable startup delay.
    metricbeat.max_start_delay: 10s

    # Setup Kibana dashboards
    setup.dashboards.enabled: true
    setup.kibana.host: "https://kibana.default.svc:443"
    setup.kibana.ssl.enabled: true
    setup.kibana.ssl.certificate_authorities: ["/usr/share/metricbeat/data/elasticsearch-ca.pem"]

    output.elasticsearch:
      # Array of hosts to connect to.
      hosts: ["https://elasticsearch-client.default.svc:443"]
      ssl.certificate_authorities: ["/usr/share/metricbeat/data/elasticsearch-ca.pem"]

      # Optional protocol and basic auth credentials.
      protocol: "https"
      username: "elastic"
      password: "es_bootstrap_password"

  metricbeat-elasticsearch-xpack.yml: |-
    # ======================== Metricbeat Modules Configuration =========================
    - module: elasticsearch
      metricsets:
        - ccr
        - enrich
        - cluster_stats
        - index
        - index_recovery
        - index_summary
        - ml_job
        - node_stats
        - shard
      period: 10s
      hosts: ["https://localhost:9200"]
      ssl.certificate_authorities: ["/usr/share/metricbeat/data/elasticsearch-ca.pem"]
      ssl.verification_mode: "full"
      username: "elastic"
      password: "es_bootstrap_password"
      xpack.enabled: true

  filebeat.yml: |-
    # ======================== Filebeat Configuration =========================
    filebeat.modules:
    - module: elasticsearch
      server:
        enabled: true
        var.paths:
          - /usr/share/elasticsearch/logs/*_server.json
      gc:
        enabled: true
        var.paths:
          - /usr/share/elasticsearch/logs/gc.log.[0-9]*
          - /usr/share/elasticsearch/logs/gc.log
      audit:
        enabled: true
        var.paths:
        - /usr/share/elasticsearch/logs/*_audit.json
      slowlog:
        enabled: true
        var.paths:
          - /usr/share/elasticsearch/logs/*_index_search_slowlog.json   
          - /usr/share/elasticsearch/logs/*_index_indexing_slowlog.json  
      deprecation:
        enabled: true
        var.paths:
          - /usr/share/elasticsearch/logs/*_deprecation.json
    output.elasticsearch:
      enabled: true
      hosts: ["https://elasticsearch-client.default.svc:443"]
      ssl.certificate_authorities: ["/usr/share/filebeat/data/elasticsearch-ca.pem"]
      protocol: "https"
      username: "elastic"
      password: "es_bootstrap_password"

    setup:
      dashboards.enabled: true
      kibana:
        host: "https://kibana.default.svc:443"
        ssl:
          enabled: true
          certificate_authorities: ["/usr/share/filebeat/data/elasticsearch-ca.pem"]

  elasticsearch.yml: |-
    # ======================== Elasticsearch Configuration =========================
    #
    # NOTE: Elasticsearch comes with reasonable defaults for most settings.
    #       Before you set out to tweak and tune the configuration, make sure you
    #       understand what are you trying to accomplish and the consequences.
    #
    # The primary way of configuring a node is via this file. This template lists
    # the most important settings you may want to configure for a production cluster.
    #
    # Please consult the documentation for further information on configuration options:
    # https://www.elastic.co/guide/en/elasticsearch/reference/index.html
    #
    # ---------------------------------- Cluster -----------------------------------
    #
    # Use a descriptive name for your cluster:
    #
    cluster.name: sandbox-cluster
    #
    # ------------------------------------ Node ------------------------------------
    #
    # Use a descriptive name for the node:
    #
    #node.name: node-1
    #
    # Add custom attributes to the node:
    #
    #node.attr.rack: r1
    #
    node.master: ${NODE_MASTER}
    node.data: ${NODE_DATA}
    node.ingest: ${NODE_INGEST}
    # ----------------------------------- Paths ------------------------------------
    #
    # Path to directory where to store the data (separate multiple locations by comma):
    #
    # ${path.data}
    #
    # Path to log files:
    #
    # ${path.logs}
    #
    # ----------------------------------- Memory -----------------------------------
    #
    # Lock the memory on startup:
    #
    #bootstrap.memory_lock: true
    #
    # Make sure that the heap size is set to about half the memory available
    # on the system and that the owner of the process is allowed to use this
    # limit.
    #
    # Elasticsearch performs poorly when the system is swapping the memory.
    #
    # ---------------------------------- Network -----------------------------------
    #
    # Set the bind address to a specific IP (IPv4 or IPv6):
    #
    network.host: 0.0.0.0
    #
    # Set a custom port for HTTP:
    #
    http.port: 9200
    #
    # For more information, consult the network module documentation.
    #
    # --------------------------------- Discovery ----------------------------------
    #
    # Pass an initial list of hosts to perform discovery when this node is started:
    # The default list of hosts is ["127.0.0.1", "[::1]"]
    #
    discovery.seed_hosts: ["{{ .Values.elasticsearch.name }}-discovery.default.svc:9300"]
    #
    # Bootstrap the cluster using an initial set of master-eligible nodes:
    #
    cluster.initial_master_nodes: "elasticsearch-master-0, elasticsearch-master-1, elasticsearch-master-2"
    #
    # For more information, consult the discovery and cluster formation module documentation.
    #
    # ---------------------------------- Gateway -----------------------------------
    #
    # Block initial recovery after a full cluster restart until N nodes are started:
    #
    #gateway.recover_after_nodes: 3
    #
    # For more information, consult the gateway module documentation.
    #
    # ---------------------------------- Various -----------------------------------
    #
    # Require explicit names when deleting indices:
    #
    #action.destructive_requires_name: true
    # ---------------------------------- Security ----------------------------------
    xpack:
      security:
        enabled: true
        transport:
          ssl:
            enabled: true
            verification_mode: none
            keystore.path: /usr/share/elasticsearch/config/certs/elasticsearch-certs.p12
            truststore.path: /usr/share/elasticsearch/config/certs/elasticsearch-certs.p12
        http:
          ssl:
            enabled: true
            truststore.path: /usr/share/elasticsearch/config/certs/elasticsearch-certs.p12
            keystore.path: /usr/share/elasticsearch/config/certs/elasticsearch-certs.p12
        authc:
          realms:
            file:
              file1:
                order: 0
    # ---------------------------------- Monitoring ----------------------------------
      monitoring:
        elasticsearch:
          collection.enabled: false
        collection.enabled: true

      #anonymous:
        #username: anonymous_user
        #roles: kibana_admin
        #authz_exception: true
       

  log4j2.properties: |-
    status = error
    property.logDir = /usr/share/elasticsearch/logs

    appender.console.type = Console
    appender.console.name = STDOUT
    appender.console.layout.type = PatternLayout
    appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c{1.}] [%node_name]%marker %m%n
    
    ######## Server JSON ############################
    appender.rolling.type = RollingFile
    appender.rolling.name = RollingFile
    appender.rolling.fileName = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}_server.json
    appender.rolling.layout.type = ESJsonLayout
    appender.rolling.layout.type_name = server

    appender.rolling.filePattern = ${sys:es.logs.base_path}${sys:file.separator}${sys:es.logs.cluster_name}-%d{yyyy-MM-dd}-%i.json.gz
    appender.rolling.policies.type = Policies
    appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
    appender.rolling.policies.time.interval = 1
    appender.rolling.policies.time.modulate = true
    appender.rolling.policies.size.type = SizeBasedTriggeringPolicy
    appender.rolling.policies.size.size = 128MB
    appender.rolling.strategy.type = DefaultRolloverStrategy
    appender.rolling.strategy.fileIndex = nomax
    appender.rolling.strategy.action.type = Delete
    appender.rolling.strategy.action.basepath = ${sys:es.logs.base_path}
    appender.rolling.strategy.action.condition.type = IfFileName
    appender.rolling.strategy.action.condition.glob = ${sys:es.logs.cluster_name}-*
    appender.rolling.strategy.action.condition.nested_condition.type = IfAccumulatedFileSize
    appender.rolling.strategy.action.condition.nested_condition.exceeds = 2GB
    ################################################
    
    appender.console.filter.threshold.type = ThresholdFilter
    appender.console.filter.threshold.level = info

    logger.console.name = STDOUT
    logger.console.level = info
    logger.console.appenderRef.console.ref = STDOUT

    logger.rolling.name = RollingFile
    logger.rolling.level = info
    logger.rolling.appenderRef.rolling.ref = RollingFile

    rootLogger.level = info
    rootLogger.appenderRef.console.ref = STDOUT
    rootLogger.appenderRef.rolling.ref = RollingFile

